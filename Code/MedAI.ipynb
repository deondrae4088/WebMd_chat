{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for loading environment variables.\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables.\n",
    "load_dotenv()\n",
    "\n",
    "# Set the model name for our LLMs.\n",
    "GEMINI_MODEL = \"gemini-1.5-flash\"\n",
    "\n",
    "# Store the API key in a variable.\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports for specific chains we'll use.\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.constitutional_ai.base import ConstitutionalChain\n",
    "from langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a medical LLM instance using the Gemini model.\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model = \"gemini-1.5-flash\",\n",
    "    temperature=1.0,\n",
    "    max_output_tokens=1000,\n",
    "    top_p=1.0,\n",
    "    google_api_key = GEMINI_API_KEY\n",
    ")\n",
    "# Create a prompt template for the LLM.\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a medical assistant.\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "# Create a chain using the LLM and the prompt template.\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template\n",
    ")\n",
    "# Define a function to get the LLM's response to a medical question.\n",
    "def get_medical_response(question: str) -> str:\n",
    "    # Use the LLM chain to get a response from the LLM.\n",
    "    response = llm_chain.run(input=question)\n",
    "    return response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get the Gemini model's response to a medical question.\n",
    "# Create a generator using the Gemini model.\n",
    "generator = ChatGoogleGenerativeAI(\n",
    "    model=GEMINI_MODEL,\n",
    "    temperature=0.0,\n",
    "    #max_output_tokens=1000,\n",
    "    top_p=1.0,\n",
    "    google_api_key=GEMINI_API_KEY\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_medical_response(prompt):\n",
    "\ttry:\n",
    "\t\t# Pass the prompt to the llm_chain to generate a response\n",
    "\t\tresults = llm_chain.run({\"input\": prompt})\n",
    "\t\t# Get the generated text directly from the results\n",
    "\t\tgenerated_text = results\n",
    "\t\t# Print the generated text.\n",
    "\t\treturn(results)\t\n",
    "\n",
    "\texcept Exception as e:\n",
    "\t\t# Handle any errors that occur during execution\n",
    "\t\treturn(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\justi\\AppData\\Local\\Temp\\ipykernel_25192\\3341170446.py:4: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  results = llm_chain.run({\"input\": prompt})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am an AI and cannot diagnose medical conditions.  A runny nose and fever could be caused by many things, from a common cold to the flu or even something more serious.  It's crucial to see a doctor or other qualified healthcare professional for a proper diagnosis and treatment plan.  They can assess your symptoms, perform any necessary tests, and determine the best course of action.\n",
      "\n",
      "In the meantime, you can try some general self-care measures to help alleviate your symptoms:\n",
      "\n",
      "* **Rest:** Get plenty of rest to help your body fight off the illness.\n",
      "* **Hydration:** Drink plenty of fluids, such as water, clear broths, or electrolyte drinks, to prevent dehydration.\n",
      "* **Over-the-counter medications:** You can take over-the-counter medications like acetaminophen (Tylenol) or ibuprofen (Advil, Motrin) to reduce fever and pain, following the dosage instructions carefully.  **Do not give aspirin to children or teenagers.**\n",
      "* **Manage nasal congestion:** Use saline nasal spray or rinse to help clear your nasal passages.\n",
      "\n",
      "**It's important to seek medical attention if:**\n",
      "\n",
      "* Your fever is very high (over 103°F or 39.4°C)\n",
      "* Your fever lasts for more than a few days\n",
      "* You have difficulty breathing\n",
      "* You have severe chest pain\n",
      "* You have a stiff neck\n",
      "* You experience any other concerning symptoms\n",
      "\n",
      "\n",
      "Please schedule an appointment with your doctor or visit an urgent care clinic as soon as possible to get the appropriate medical care.  I cannot provide medical advice, only information.\n"
     ]
    }
   ],
   "source": [
    "# create a variable to return the function\n",
    "prompt = \"MedAI, I'm sick, I have a runny nose, and a fever, what is it and what do I do?\"\n",
    "response = get_medical_response(prompt)\n",
    "# Print the response from the model\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (5.23.0)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from gradio) (4.2.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from gradio) (0.115.12)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from gradio) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.8.0 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from gradio) (1.8.0)\n",
      "Requirement already satisfied: groovy~=0.1 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from gradio) (0.27.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.28.1 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from gradio) (0.29.2)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from gradio) (3.1.4)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from gradio) (1.26.4)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from gradio) (3.10.16)\n",
      "Requirement already satisfied: packaging in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from gradio) (24.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from gradio) (2.2.2)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from gradio) (10.4.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from gradio) (2.10.6)\n",
      "Requirement already satisfied: pydub in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from gradio) (6.0.1)\n",
      "Requirement already satisfied: ruff>=0.9.3 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from gradio) (0.11.2)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from gradio) (0.46.1)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from gradio) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from gradio) (0.15.2)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from gradio) (4.12.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from gradio) (0.34.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from gradio-client==1.8.0->gradio) (2024.6.1)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio) (3.13.1)\n",
      "Requirement already satisfied: requests in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio) (4.66.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.15.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.2.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\justi\\anaconda3\\envs\\ai\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* Running on public URL: https://fe898cdfc3b13affd5.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://fe898cdfc3b13affd5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install the gradio package if not already installed\n",
    "%pip install gradio\n",
    "\n",
    "#create the Gradio interface\n",
    "import gradio as gr\n",
    "# Create a Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=get_medical_response,\n",
    "    inputs= [gr.Textbox(label=\"Ask Me A Medical Question!\")],\n",
    "    outputs=gr.Textbox(lines=10, label=\"Medical Response\"),\n",
    "    title=\"Medical Assistant\"\n",
    ")\n",
    "# Launch the Gradio interface\n",
    "interface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gradio interface\n",
    "\n",
    "# Gradio call the function\n",
    "\n",
    "# create the Gradio output component\n",
    "\n",
    "\n",
    "# Define a small generator function using the existing llm instance.\n",
    "def small_generator(prompt, max_length=25, pad_token_id=50256):\n",
    "\n",
    "\t# Generate a response using the llm instance.\n",
    "\tresponse = llm_chain.run({\"input\": prompt})\n",
    "\treturn [{\"generated_text\": response}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Invalid argument provided to Gemini: 400 API key not valid. Please pass a valid API key. [reason: \"API_KEY_INVALID\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"API key not valid. Please pass a valid API key.\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Give the model a prompt. \n",
    "prompt = \"What are the possible causes for a topical rash?\"\n",
    "try:\n",
    "    # Pass the prompt to the small_generator to generate a response\n",
    "    results = small_generator(prompt, max_length=100, pad_token_id=50256)\n",
    "    # Get the generated text directly from the results\n",
    "    generated_text = results[0]['generated_text']\n",
    "    # Print the generated text.\n",
    "    print(generated_text) \n",
    "except Exception as e:\n",
    "    # Handle any errors that occur during execution\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Invalid argument provided to Gemini: 400 API key not valid. Please pass a valid API key. [reason: \"API_KEY_INVALID\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"API key not valid. Please pass a valid API key.\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What are the leading causes of cancer in the world?\"\n",
    "try:\n",
    "    # Pass the prompt to the small_generator to generate a response\n",
    "    results = small_generator(prompt, max_length=100, pad_token_id=50256)\n",
    "    # Get the generated text directly from the results\n",
    "    generated_text = results[0]['generated_text']\n",
    "    # Print the generated text.\n",
    "    print(generated_text)\n",
    "except Exception as e:\n",
    "    # Handle any errors that occur during execution\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pandas \n",
    "import pandas as pd\n",
    "\n",
    "# Create a function to generate the answers based on an input text.\n",
    "def question_answer(questions, text):\n",
    "    # Create a list to hold the data that will be added to the DataFrame.\n",
    "    data = []\n",
    "    # Use a for loop to iterate through the questions.\n",
    "    for question in questions:\n",
    "        # Pass the question and text to the initialized question_answerer. \n",
    "        result = question_answerer(question=question, context=text)\n",
    "        # Retrieve the question, answer, the score, the starting \n",
    "        # and ending of where the answer is located in the text.\n",
    "        data.append([question, result['answer'], result['score'], result['start'], result['end']])\n",
    "    # Create a DataFrame from the data with appropriate columns. \n",
    "    df = pd.DataFrame(data, columns=[\"Question\", \"Answer\", \"Score\", \"Starting Position\", \"Ending Position\"])\n",
    "    # Return the DataFrame\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
