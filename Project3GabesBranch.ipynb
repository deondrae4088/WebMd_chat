{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGJwosScpe+6m0A8CN5Jzr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deondrae4088/WebMd_chat/blob/Project-3-(Gabe's-Branch)/Project3GabesBranch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrj-39a7VBuH"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade tensorflow kagglehub\n",
        "import tensorflow as tf\n",
        "import pathlib\n",
        "import kagglehub\n",
        "import os\n",
        "import zipfile\n",
        "import pathlib\n",
        "import glob\n",
        "import PIL\n",
        "\n",
        "dataset_names = [\n",
        "    \"shubhamgoel27/dermnet\",\n",
        "    \"pkdarabi/brain-tumor-image-dataset-semantic-segmentation\",\n",
        "    \"paultimothymooney/chest-xray-pneumonia\",\n",
        "    \"maedemaftouni/covid19-ct-scan-lesion-segmentation-dataset\",\n",
        "    \"aryashah2k/breast-ultrasound-images-dataset\"\n",
        "]\n",
        "\n",
        "extracted_dirs = []\n",
        "\n",
        "for name in dataset_names:\n",
        "    try:\n",
        "        download_path = kagglehub.dataset_download(name)\n",
        "        print(f\"Downloaded '{name}' to: {download_path}\")\n",
        "        extract_path = os.path.join(os.getcwd(), name.split('/')[1]) if download_path.endswith(\".zip\") else download_path\n",
        "        if download_path.endswith(\".zip\"):\n",
        "            with zipfile.ZipFile(download_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(extract_path)\n",
        "            print(f\"Extracted '{name}' to: {extract_path}\")\n",
        "        extracted_dirs.append(pathlib.Path(extract_path))\n",
        "    except Exception as e:\n",
        "        print(f\"Error with '{name}': {e}\")\n",
        "\n",
        "[print(f\"Processing: {img_dir}\") for img_dir in extracted_dirs]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "all_images = []\n",
        "\n",
        "def find_images(img_dir):\n",
        "    print(f\"Processing: {img_dir}\")\n",
        "    image_patterns = [str(img_dir / '**/*.jpg'), str(img_dir / '**/*.jpeg'), str(img_dir / '**/*.png')]\n",
        "    dataset_images = [pathlib.Path(f) for pattern in image_patterns for f in glob.glob(pattern, recursive=True)]\n",
        "    print(f\"  Found {len(dataset_images)} images\")\n",
        "    return dataset_images\n",
        "\n",
        "list_of_image_lists = list(map(find_images, extracted_dirs))\n",
        "all_images = [img for sublist in list_of_image_lists for img in sublist]\n",
        "\n",
        "print(f\"\\nTotal images found: {len(all_images)}\")\n",
        "\n",
        "# Display the first image if any were found across all datasets\n",
        "if all_images:\n",
        "    first_image_path = all_images[0]\n",
        "    print(f\"First image found: {first_image_path}\")\n",
        "    try:\n",
        "        img = PIL.Image.open(str(first_image_path))\n",
        "        img.show()  # This will display the image in a new window\n",
        "        print(f\"Image size: {img.size}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error opening image: {e}\")\n",
        "else:\n",
        "    print(\"No images found across any of the datasets. Check dataset structures.\")"
      ],
      "metadata": {
        "id": "yp3bOF0HVWVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "from collections import defaultdict\n",
        "\n",
        "def organize_images_by_class(root_dir, image_extensions=['*.jpg', '*.jpeg', '*.png']):\n",
        "    \"\"\"Organizes images by class based on subdirectory names.\"\"\"\n",
        "    root_path = pathlib.Path(root_dir)\n",
        "    return defaultdict(list, {\n",
        "        item.name: [p for ext in image_extensions for p in item.glob(ext)]\n",
        "        for item in root_path.iterdir() if item.is_dir()\n",
        "    })\n",
        "\n",
        "if extracted_dirs:\n",
        "    all_organized_images = defaultdict(list)\n",
        "    list(map(lambda organized_dataset: [all_organized_images[cls].extend(imgs)\n",
        "                                         for cls, imgs in organized_dataset.items()],\n",
        "             map(organize_images_by_class, extracted_dirs)))\n",
        "\n",
        "    print(\"\\nOrganized images by class:\") if all_organized_images else print(\"\\nNo images organized by class found.\")\n",
        "    list(map(lambda item: print(f\"Class: {item[0]}, Found {len(item[1])} images\"),\n",
        "             all_organized_images.items()))\n",
        "else:\n",
        "    print(\"\\nNo dataset directories found.\")"
      ],
      "metadata": {
        "id": "ui7ZsMC7VbVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model\n",
        "model = Sequential([\n",
        "    layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
        "    layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Flatten(),  # Corrected from flatten() to Flatten()\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Print model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "IpnyuvpYV1fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'all_organized_images' dictionary from the organization step\n",
        "\n",
        "image_dir = pathlib.Path(\"./\") # Replace with the actual root directory of your datasets\n",
        "\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    image_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    image_size=(img_height, img_width),\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=123,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    image_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    image_size=(img_height, img_width),\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=123,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# Optional: Configure performance for faster data loading\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=10)\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(test_ds)\n",
        "print(f\"Test accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "nVW-L1bIWCl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a simple Gradio app that uses a *trained* model to classify images\n",
        "# based on the labels it learned during training.\n",
        "import gradio as gr\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# --- Assume you have a trained model and class labels ---\n",
        "# Replace these placeholders with your actual trained model and labels\n",
        "\n",
        "# Example of a simple CNN model (replace with your actual model)\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.fc = nn.Linear(32 * 56 * 56, num_classes) # Adjust input size based on your training image size\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.maxpool(self.relu(self.conv1(x)))\n",
        "        x = self.maxpool(self.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 32 * 56 * 56) # Flatten\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Assume you have the number of classes from your training data\n",
        "num_classes = 10  # Replace with the actual number of classes\n",
        "\n",
        "# Load your trained model\n",
        "model = SimpleCNN(num_classes)\n",
        "# Replace 'path/to/your/trained_model.pth' with the actual path to your saved model\n",
        "try:\n",
        "    model.load_state_dict(torch.load('path/to/your/trained_model.pth'))\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    print(\"Trained model loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Trained model file not found. Please provide the correct path.\")\n",
        "    exit()\n",
        "\n",
        "# Assume you have the class labels corresponding to the model's output\n",
        "# Replace this with your actual list of class labels\n",
        "class_labels = ['class_0', 'class_1', 'class_2', 'class_3', 'class_4',\n",
        "                'class_5', 'class_6', 'class_7', 'class_8', 'class_9']\n",
        "\n",
        "# Define the transformations used during training\n",
        "# Make sure these are the *same* transformations you applied to your training data\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Or the size you trained on\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Or your training normalization\n",
        "])\n",
        "\n",
        "def classify_image(img):\n",
        "    \"\"\"Classifies the input image using the loaded trained model.\"\"\"\n",
        "    try:\n",
        "        img = Image.fromarray(img).convert('RGB') # Ensure image is RGB\n",
        "        img_t = transform(img).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model(img_t)\n",
        "            probabilities = F.softmax(out, dim=1)\n",
        "            _, predicted_idx = torch.max(probabilities, 1)\n",
        "            predicted_label = class_labels[predicted_idx.item()]\n",
        "            confidence = probabilities[0, predicted_idx.item()].item()\n",
        "\n",
        "        return f\"Predicted Class: {predicted_label} (Confidence: {confidence:.4f})\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error during classification: {e}\"\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=classify_image,\n",
        "    inputs=gr.Image(type=\"numpy\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Image Classification with Trained Model\",\n",
        "    description=\"Upload an image to classify it based on the model it was trained on.\"\n",
        ")\n",
        "\n",
        "demo.launch()\n",
        ")"
      ],
      "metadata": {
        "id": "QSFtCgpnWGoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Create a Gradio interface for the image classification model\n",
        "demo= gr.Interface(\n",
        "    fn=lambda img: img. inputs= \"image\"\n",
        "    outputs=\"text\",\n",
        "    title=\"Image Classification Demo\", )\n",
        "launch(show_error=True)\n",
        "app.launch(show_error=True)"
      ],
      "metadata": {
        "id": "fLgw0G5lWIOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Define a custom theme for the Gradio app\n",
        "gr.themes.builder()\n",
        "class Seafoam (Base):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.background_color = \"#008080\"\n",
        "        self.text_color = \"#000000\"\n",
        "        self.button_color = \"#800020\"\n",
        "        self.button_text_color = \"#000000\"\n",
        "        self.input_background_color = \"#FFFFFF\"\n",
        "        self.input_text_color = \"#000000\"\n",
        "        self.output_background_color = \"#FFFFFF\"\n",
        "        self.output_text_color = \"#000000\"\n",
        "#   Create a Gradio Blocks app with the custom theme\n",
        "    with gr.Blocks(theme=Seafoam(primary_hue=\"008080\", secondary_hue=\"800020\")) as demo:\n",
        "        gr.Markdown(\"# Image Classification with ResNet50\")\n",
        "        gr.Markdown(\"Upload an image to classify it.\")\n",
        "        gr.Markdown (\"ask a medical question\")\n",
        "        image_input = gr.Image(type=\"pil\", label=\"Input Image\")\n",
        "        image_classify_button = gr.Button(\"Classify Image\")\n",
        "        output_text = gr.Textbox(label=\"Classification Result\")\n",
        "        textbox = gr.Textbox(label=\"Ask a medical question\")\n",
        "        text_classify_button = gr.Button(\"Ask\")\n",
        "        text_classify_button.click(predict_fn, inputs=image_input, outputs=output_text)\n",
        "        text_classify_button.click(predict_fn, inputs=textbox, outputs=output_text)\n",
        "# Set spacing and radius sizes for the Blocks app\n",
        "with gr.Blocks(theme=gr.themes.Default(spacing_size=\"md\", radius_size=\"md\")) as demo:\n",
        "# Set fonts for the Blocks app\n",
        "with gr.Blocks(theme=gr.themes.Default(font=[gr.themes.GoogleFont(\"Helvetica\"), \"Arial\", \"sans-serif\"])) as demo:"
      ],
      "metadata": {
        "id": "H1iuTbxcWMGE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}